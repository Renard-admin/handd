<!DOCTYPE html>
<html>
<head>
    <title>AI Vision Demo</title>
    <style>
        body { margin: 0; overflow: hidden; }
        #video-container { position: relative; }
        #video { transform: scaleX(-1); }
        .canvas-overlay { 
            position: absolute;
            top: 0;
            left: 0;
        }
        .control-panel {
            position: fixed;
            top: 10px;
            right: 10px;
            z-index: 1000;
        }
        button {
            padding: 8px;
            margin: 4px;
            background: #444;
            color: white;
            border: none;
            cursor: pointer;
        }
    </style>
</head>
<body>
    <div class="control-panel">
        <button id="fullscreen">Fullscreen</button>
        <button id="switchCamera">Switch Camera</button>
    </div>
    <div id="video-container">
        <video id="video" autoplay playsinline></video>
        <canvas id="handCanvas" class="canvas-overlay"></canvas>
        <canvas id="faceCanvas" class="canvas-overlay"></canvas>
        <canvas id="objectCanvas" class="canvas-overlay"></canvas>
        <canvas id="drawCanvas" class="canvas-overlay"></canvas>
    </div>

    <!-- Libraries -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/face-api.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>

    <script>
        // Camera and video setup
        let video = document.getElementById('video');
        let currentStream;
        let isFrontCamera = true;
        
        // Canvases
        const handCanvas = document.getElementById('handCanvas');
        const faceCanvas = document.getElementById('faceCanvas');
        const objectCanvas = document.getElementById('objectCanvas');
        const drawCanvas = document.getElementById('drawCanvas');
        const canvases = [handCanvas, faceCanvas, objectCanvas, drawCanvas];
        
        // State
        let isDrawingMode = false;
        let lastGesture = '';
        let gestureCount = 0;
        let lastPoint = null;

        async function setupCamera() {
            const constraints = {
                video: { 
                    facingMode: isFrontCamera ? 'user' : 'environment',
                    width: { ideal: 1280 },
                    height: { ideal: 720 }
                }
            };
            
            try {
                currentStream = await navigator.mediaDevices.getUserMedia(constraints);
                video.srcObject = currentStream;
                await new Promise(resolve => video.onloadedmetadata = resolve);
                
                canvases.forEach(canvas => {
                    canvas.width = video.videoWidth;
                    canvas.height = video.videoHeight;
                });
            } catch(error) {
                console.error('Camera error:', error);
            }
        }

        // MediaPipe Hands setup
        const hands = new Hands({
            locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`
        });
        
        hands.setOptions({
            maxNumHands: 2,
            modelComplexity: 1,
            minDetectionConfidence: 0.5,
            minTrackingConfidence: 0.5
        });
        
        hands.onResults(processHands);

        // Face-API setup
        async function loadFaceModels() {
            await faceapi.nets.tinyFaceDetector.loadFromUri('/models');
            await faceapi.nets.ageGenderNet.loadFromUri('/models');
            await faceapi.nets.faceExpressionNet.loadFromUri('/models');
        }

        // COCO-SSD setup
        let cocoModel;
        async function loadCocoModel() {
            cocoModel = await cocoSsd.load();
        }

        // Fullscreen and camera switching
        document.getElementById('fullscreen').onclick = () => 
            document.documentElement.requestFullscreen();
            
        document.getElementById('switchCamera').onclick = () => {
            isFrontCamera = !isFrontCamera;
            setupCamera();
        };

        // Main processing loop
        async function processFrame() {
            // Process hands
            await hands.send({image: video});
            
            // Process faces
            const faceCtx = faceCanvas.getContext('2d');
            faceCtx.clearRect(0, 0, faceCanvas.width, faceCanvas.height);
            
            const detections = await faceapi.detectAllFaces(video, 
                new faceapi.TinyFaceDetectorOptions())
                .withAgeAndGender()
                .withFaceExpressions();
            
            detections.forEach(detection => {
                const box = detection.detection.box;
                faceCtx.strokeStyle = 'yellow';
                faceCtx.lineWidth = 2;
                faceCtx.strokeRect(box.x, box.y, box.width, box.height);
                
                // Display age and emotion
                faceCtx.fillStyle = 'yellow';
                faceCtx.fillText(
                    `Age: ${Math.round(detection.age)}, ${detection.gender} | ` +
                    `Emotion: ${detection.expressions.asSortedArray()[0].expression}`,
                    box.x, box.y - 5
                );
            });

            // Process objects
            const objectCtx = objectCanvas.getContext('2d');
            objectCtx.clearRect(0, 0, objectCanvas.width, objectCanvas.height);
            
            const predictions = await cocoModel.detect(video);
            predictions.forEach(prediction => {
                const [x, y, width, height] = prediction.bbox;
                let color = 'yellow';
                let label = prediction.class;
                
                if(['knife', 'gun', 'scissors'].includes(prediction.class)) {
                    color = 'red';
                    label = `DANGER: ${label}`;
                } else if(['apple', 'banana', 'pizza'].includes(prediction.class)) {
                    color = 'green';
                    label = `Food: ${label}`;
                }
                
                objectCtx.strokeStyle = color;
                objectCtx.lineWidth = 2;
                objectCtx.strokeRect(x, y, width, height);
                objectCtx.fillStyle = color;
                objectCtx.fillText(label, x, y > 10 ? y - 5 : 10);
            });

            requestAnimationFrame(processFrame);
        }

        function processHands(results) {
            const ctx = handCanvas.getContext('2d');
            ctx.clearRect(0, 0, handCanvas.width, handCanvas.height);
            
            if (results.multiHandLandmarks) {
                results.multiHandLandmarks.forEach(landmarks => {
                    // Draw hand bounding box
                    const xs = landmarks.map(l => l.x * video.videoWidth);
                    const ys = landmarks.map(l => l.y * video.videoHeight);
                    const x = Math.min(...xs);
                    const y = Math.min(...ys);
                    const width = Math.max(...xs) - x;
                    const height = Math.max(...ys) - y;
                    
                    ctx.strokeStyle = 'green';
                    ctx.lineWidth = 2;
                    ctx.strokeRect(x, y, width, height);

                    // Gesture recognition
                    detectGestures(landmarks);
                });
            }
        }

        function detectGestures(landmarks) {
            // Implement gesture detection logic
            // (Simplified example - needs proper implementation)
            const thumbTip = landmarks[4];
            const indexTip = landmarks[8];
            const middleTip = landmarks[12];
            
            // Check for "OK" gesture
            const distance = Math.hypot(
                thumbTip.x - indexTip.x,
                thumbTip.y - indexTip.y
            );
            
            if(distance < 0.05) {
                // Clear drawing
                const ctx = drawCanvas.getContext('2d');
                ctx.clearRect(0, 0, drawCanvas.width, drawCanvas.height);
                return;
            }

            // Other gesture detection logic...
        }

        // Initialization
        (async function main() {
            await setupCamera();
            await loadFaceModels();
            await loadCocoModel();
            processFrame();
        })();
    </script>
</body>
</html>
